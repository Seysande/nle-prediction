{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seysande/nle-prediction/blob/main/nle_evaluation_XGBoost_SMOTE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L1SCF8q9Y6o"
      },
      "source": [
        "# Load Data and Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reset"
      ],
      "metadata": {
        "id": "3WulriAZm97D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6XScONyNvJK"
      },
      "outputs": [],
      "source": [
        "!pip install rdkit-pypi\n",
        "!pip install bayesian-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOhCdcNSJYkQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, DataStructs\n",
        "import xgboost as xgb\n",
        "import seaborn as sns\n",
        "\n",
        "# Data Preprocessing\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, ConfusionMatrixDisplay, confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve, auc, precision_score, recall_score, average_precision_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "df = pd.read_csv('drugData_jan-2024.csv')\n",
        "smiles_list = df['smiles'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xgb.__version__)"
      ],
      "metadata": {
        "id": "uOogTeCp8gqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRWGksMvONOS"
      },
      "outputs": [],
      "source": [
        "def smiles_to_mol(smiles):\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            raise ValueError(f\"Invalid SMILES string: {smiles}\")\n",
        "        return mol\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return None\n",
        "\n",
        "molecules = [smiles_to_mol(smiles) for smiles in smiles_list]\n",
        "molecules = [mol for mol in molecules if mol is not None]\n",
        "fingerprints = [AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048) for mol in molecules]\n",
        "df['fingerprint'] = fingerprints\n",
        "df = df.drop('smiles', axis=1)\n",
        "\n",
        "X = np.array(list(df['fingerprint']))\n",
        "y = df['label']\n",
        "y = np.array(y)\n",
        "\n",
        "np.shape(X)\n",
        "display(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqGzuvT3-Z6C"
      },
      "source": [
        "# Training and Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POH0CswgTq-L"
      },
      "outputs": [],
      "source": [
        "def xgb_eval(n_estimators, eta, max_depth, min_child_weight, scale_pos_weight, subsample, colsample_bytree, max_delta_step, X_train, y_train, X_test, y_test):\n",
        "  n_estimators = int(n_estimators)\n",
        "  max_depth = int(max_depth)\n",
        "  subsample = max(0, min(1, subsample))\n",
        "  colsample_bytree = max(0.5, min(1, colsample_bytree))\n",
        "  min_child_weight = int(min_child_weight)\n",
        "\n",
        "  # Defining a dictionary of parameters\n",
        "  params = {\n",
        "      'n_estimators': n_estimators,\n",
        "      'eta': eta,\n",
        "      'max_depth': max_depth,\n",
        "      'min_child_weight': min_child_weight,\n",
        "      'scale_pos_weight': scale_pos_weight,\n",
        "      'subsample': subsample,\n",
        "      'colsample_bytree': colsample_bytree,\n",
        "      'max_delta_step': max_delta_step,\n",
        "      'eval_metric':'aucpr',\n",
        "      'objective': 'binary:logistic',\n",
        "      'use_label_encoder':False,\n",
        "  }\n",
        "  # Set up parameters for XGBoost\n",
        "  model = xgb.XGBClassifier(**params)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "  precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "  pr_auc = auc(recall, precision)\n",
        "\n",
        "  return pr_auc\n",
        "\n",
        "#\n",
        "param_bounds = {\n",
        "    'n_estimators': (50, 1000),\n",
        "    'eta': (0.01, 0.1),\n",
        "    'max_depth': (3, 7),\n",
        "    'min_child_weight': (1, 10),\n",
        "    'scale_pos_weight':(9, 11),\n",
        "    'subsample': (0.6, 0.9),\n",
        "    'colsample_bytree': (0.6, 0.9),\n",
        "    'max_delta_step': (0, 1),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzMZ-7KgfaE-"
      },
      "outputs": [],
      "source": [
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "precision_values = []\n",
        "recall_values = []\n",
        "average_precisions = []\n",
        "fprs = []\n",
        "tprs = []\n",
        "confusion_matrices = []\n",
        "accuracies = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "\n",
        "  smote = SMOTE(random_state=42)\n",
        "  X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Take x_train, y_train and split 80/20\n",
        "  X_train_res, X_val, y_train_res, y_val = train_test_split(X_train_res, y_train_res, test_size = 0.2, random_state = 42)\n",
        "\n",
        "  def xgb_eval_curr(n_estimators, eta, min_child_weight, scale_pos_weight, max_depth, max_delta_step, subsample, colsample_bytree):\n",
        "    return xgb_eval(n_estimators, eta, min_child_weight, scale_pos_weight, max_depth, max_delta_step, subsample, colsample_bytree, X_train_res, y_train_res, X_test, y_test)\n",
        "\n",
        "# Hyperparameter Optimization\n",
        "  optimizer = BayesianOptimization(f=xgb_eval_curr, pbounds=param_bounds, random_state=42, verbose=1)\n",
        "  optimizer.maximize(init_points=16, n_iter=5)\n",
        "\n",
        "# Store best parameters\n",
        "  best_params = optimizer.max['params']\n",
        "  best_params['max_depth'] = int(best_params['max_depth'])\n",
        "  best_params['n_estimators'] = int(best_params['n_estimators'])\n",
        "  best_params['min_child_weight'] = int(best_params['min_child_weight'])\n",
        "  best_params['subsample'] = max(0, min(1, best_params['subsample']))\n",
        "  best_params['colsample_bytree'] = max(0.5, min(1, best_params['colsample_bytree']))\n",
        "\n",
        "\n",
        "  dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "  dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "  num_rounds=100\n",
        "\n",
        "  bst = xgb.train(best_params, dtrain, num_rounds)\n",
        "  y_pred_prob = bst.predict(dtest)\n",
        "  y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "  precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
        "  average_precision = average_precision_score(y_test, y_pred_prob)\n",
        "\n",
        "  fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "  roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "\n",
        "  # AUC values for each graph\n",
        "  fprs.append(np.interp(np.linspace(0, 1, 100), fpr, tpr))\n",
        "  tprs.append(np.linspace(0, 1, 100))\n",
        "\n",
        "  # Store PR values for the graph\n",
        "  precision_values.append(np.interp(np.linspace(0, 1, 100), recall[::-1], precision[::-1]))\n",
        "  recall_values.append(np.linspace(0, 1, 100))\n",
        "  average_precisions.append(average_precision)\n",
        "\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  confusion_matrices.append(cm)\n",
        "\n",
        "\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  accuracies.append(accuracy)\n",
        "\n",
        "  plt.plot(recall, precision, lw=2, label=f'Fold {fold_idx+1} (area = {average_precision:.2f})')\n",
        "\n",
        "\n",
        "mean_confusion_matrix = np.mean(confusion_matrices, axis=0)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(mean_confusion_matrix, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.title('Mean Confusion Matrix')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "mean_precision = np.mean(precision_values, axis=0)\n",
        "mean_recall = np.mean(recall_values, axis=0)\n",
        "mean_fpr = np.mean(fprs, axis=0)\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "\n",
        "mean_accuracy = np.mean(accuracies)\n",
        "mean_average_precision = np.mean(average_precision)\n",
        "print(f\"Mean Average Precision: {mean_average_precision:.2f}\")\n",
        "print(f\"Mean Accuracy: {mean_accuracy:.2f}\")\n",
        "\n",
        "plt.plot(mean_recall, mean_precision, 'k--', lw=2, label = f'Mean (area = {mean_average_precision:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(mean_tpr, mean_fpr, lw=2, label=f'Mean (area = {mean_accuracy:.2f})')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(mean_recall, mean_precision, lw=2, color=\"purple\", label = f'Mean (area = {mean_average_precision:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim(0, 1)\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T9I6utw26gbs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3L1SCF8q9Y6o"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}